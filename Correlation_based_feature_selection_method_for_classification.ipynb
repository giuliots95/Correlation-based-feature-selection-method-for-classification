{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c82903",
   "metadata": {},
   "source": [
    "# Correlation based feature selection method for classification\n",
    "This notebook contains an implementation of a feature selection method, based only on the dataset variables correlation properties. \n",
    "Feature selection and dimensionality reduction are common tasks in data analysis and are usually performed before developing Machine Learning models. In many applications, given a dataset with a lot of variables, a data scientist needs to understand which input features are more significant to predict the desired outcome (the outcome could be a continuous variable in the case of regression, or a discrete variable in the case of classification) and which one are irrelevant, or at least less significant and can be neglected in the modelling phase. Moreover, dimensionality reduction methods allows to simplify the problem structure, thus improving the generalization capability of the mathematical model (we simply expect to reduce the underlaying model complexity). Another advantage of reducing the problem dimension is the decrease of the computational cost during training and validation phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7221a4",
   "metadata": {},
   "source": [
    "## Madelon dataset\n",
    "In this notebook, we will apply the proposed method to the Madelon dataset, obtaining a subset of the most \"important\" features.\n",
    "Madelon is a benchmark artificial dataset used to simulate a high-dimension classification problem. We are dealing with a binary classification problem with 500 continuous input features and 1 array of class labels. Class labels are {-1, +1}. \n",
    "Original feature names are integers, ranging from 0 up to 499."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3657f7",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "The goal of this study is to reduce the dimensionality of the Madelon problem to an \"acceptable\" number of input features, dropping the less significant variables. So we want to apply a filtering technique.\n",
    "\n",
    "In \"sklearn\" package there are many feature selection methods that can be used to perform this task. However, common filtering techniques are based on the results of some mathematical models like Lasso regressor or decision trees, that have been previously fit to the dataset. \n",
    "Another limitation of some methods are that we have embedded hyper-parameters that have to be tuned, or that we should know a priori the number of significant features from the entire collection of features.\n",
    "\n",
    "The proposed method instead, is based only on the feature-to-feature correlation and the feature-to-target correlation properties. In few words, only information from the data are used and there is no need of training and validating a mathematical model for feature selection. More in detail, this method does not make any assumption on the number of features to be selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9f1ac",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Following packages are needed to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6293206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pointbiserialr\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fef659",
   "metadata": {},
   "source": [
    "## Import the dataset \n",
    "Madelon dataset can be directly imported by UCI repository, with Pandas read_csv. Otherwise, one could save the dataset manually in the notebook folder and import the data files in a similar way. \n",
    "Input features are contained in the 'madelon_train.data' file, while the outcome (so the array of labels) is in the 'madelon_train.data' file.\n",
    "\n",
    "Please note that getting data from the URLs may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953bb180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has originally 2000 rows and 501 columns\n"
     ]
    }
   ],
   "source": [
    "uci_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.data'\n",
    "uci_target_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.labels'\n",
    "\n",
    "uci_target = pd.read_csv(uci_target_url, header = None) # assigning the target data to 'uci_target'\n",
    "\n",
    "uci_train = pd.read_csv(uci_train_url, delimiter=' ', header=None)\n",
    "print(\"Training set has originally {} rows and {} columns\".format(uci_train.shape[0], uci_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4f28e",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "We can drop the column number 500, since it contains only NaN entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656c1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_train.drop(columns = 500, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3f272",
   "metadata": {},
   "source": [
    "It is a good practice to rename the dataframes column usin a string type, rather than integer type. Following loop renames the variables from \"V000\" until \"V499\", preserving the number of 500 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea9edf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V001</th>\n",
       "      <th>V002</th>\n",
       "      <th>V003</th>\n",
       "      <th>V004</th>\n",
       "      <th>V005</th>\n",
       "      <th>V006</th>\n",
       "      <th>V007</th>\n",
       "      <th>V008</th>\n",
       "      <th>V009</th>\n",
       "      <th>V0010</th>\n",
       "      <th>...</th>\n",
       "      <th>V491</th>\n",
       "      <th>V492</th>\n",
       "      <th>V493</th>\n",
       "      <th>V494</th>\n",
       "      <th>V495</th>\n",
       "      <th>V496</th>\n",
       "      <th>V497</th>\n",
       "      <th>V498</th>\n",
       "      <th>V499</th>\n",
       "      <th>V500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>477</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483</td>\n",
       "      <td>458</td>\n",
       "      <td>460</td>\n",
       "      <td>487</td>\n",
       "      <td>587</td>\n",
       "      <td>475</td>\n",
       "      <td>526</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>469</td>\n",
       "      <td>...</td>\n",
       "      <td>463</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>338</td>\n",
       "      <td>513</td>\n",
       "      <td>486</td>\n",
       "      <td>483</td>\n",
       "      <td>492</td>\n",
       "      <td>510</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>487</td>\n",
       "      <td>542</td>\n",
       "      <td>499</td>\n",
       "      <td>468</td>\n",
       "      <td>448</td>\n",
       "      <td>471</td>\n",
       "      <td>442</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>477</td>\n",
       "      <td>...</td>\n",
       "      <td>487</td>\n",
       "      <td>481</td>\n",
       "      <td>492</td>\n",
       "      <td>650</td>\n",
       "      <td>506</td>\n",
       "      <td>501</td>\n",
       "      <td>480</td>\n",
       "      <td>489</td>\n",
       "      <td>499</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>485</td>\n",
       "      <td>495</td>\n",
       "      <td>472</td>\n",
       "      <td>417</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>476</td>\n",
       "      <td>...</td>\n",
       "      <td>491</td>\n",
       "      <td>480</td>\n",
       "      <td>474</td>\n",
       "      <td>572</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>484</td>\n",
       "      <td>502</td>\n",
       "      <td>528</td>\n",
       "      <td>489</td>\n",
       "      <td>466</td>\n",
       "      <td>481</td>\n",
       "      <td>402</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>468</td>\n",
       "      <td>...</td>\n",
       "      <td>488</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>435</td>\n",
       "      <td>486</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>504</td>\n",
       "      <td>495</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>490</td>\n",
       "      <td>505</td>\n",
       "      <td>503</td>\n",
       "      <td>474</td>\n",
       "      <td>463</td>\n",
       "      <td>461</td>\n",
       "      <td>519</td>\n",
       "      <td>476</td>\n",
       "      <td>518</td>\n",
       "      <td>467</td>\n",
       "      <td>...</td>\n",
       "      <td>467</td>\n",
       "      <td>479</td>\n",
       "      <td>449</td>\n",
       "      <td>588</td>\n",
       "      <td>499</td>\n",
       "      <td>506</td>\n",
       "      <td>475</td>\n",
       "      <td>463</td>\n",
       "      <td>507</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>480</td>\n",
       "      <td>475</td>\n",
       "      <td>476</td>\n",
       "      <td>480</td>\n",
       "      <td>495</td>\n",
       "      <td>482</td>\n",
       "      <td>515</td>\n",
       "      <td>479</td>\n",
       "      <td>480</td>\n",
       "      <td>484</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>474</td>\n",
       "      <td>473</td>\n",
       "      <td>424</td>\n",
       "      <td>454</td>\n",
       "      <td>570</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>465</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>480</td>\n",
       "      <td>517</td>\n",
       "      <td>631</td>\n",
       "      <td>470</td>\n",
       "      <td>485</td>\n",
       "      <td>474</td>\n",
       "      <td>535</td>\n",
       "      <td>476</td>\n",
       "      <td>493</td>\n",
       "      <td>466</td>\n",
       "      <td>...</td>\n",
       "      <td>501</td>\n",
       "      <td>483</td>\n",
       "      <td>479</td>\n",
       "      <td>687</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>483</td>\n",
       "      <td>500</td>\n",
       "      <td>523</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>484</td>\n",
       "      <td>481</td>\n",
       "      <td>505</td>\n",
       "      <td>478</td>\n",
       "      <td>542</td>\n",
       "      <td>477</td>\n",
       "      <td>518</td>\n",
       "      <td>477</td>\n",
       "      <td>510</td>\n",
       "      <td>472</td>\n",
       "      <td>...</td>\n",
       "      <td>487</td>\n",
       "      <td>483</td>\n",
       "      <td>526</td>\n",
       "      <td>750</td>\n",
       "      <td>486</td>\n",
       "      <td>529</td>\n",
       "      <td>484</td>\n",
       "      <td>473</td>\n",
       "      <td>527</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>474</td>\n",
       "      <td>493</td>\n",
       "      <td>469</td>\n",
       "      <td>486</td>\n",
       "      <td>521</td>\n",
       "      <td>475</td>\n",
       "      <td>494</td>\n",
       "      <td>479</td>\n",
       "      <td>481</td>\n",
       "      <td>473</td>\n",
       "      <td>...</td>\n",
       "      <td>467</td>\n",
       "      <td>476</td>\n",
       "      <td>508</td>\n",
       "      <td>449</td>\n",
       "      <td>463</td>\n",
       "      <td>533</td>\n",
       "      <td>481</td>\n",
       "      <td>489</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V001  V002  V003  V004  V005  V006  V007  V008  V009  V0010  ...  V491  \\\n",
       "0      485   477   537   479   452   471   491   476   475    473  ...   477   \n",
       "1      483   458   460   487   587   475   526   479   485    469  ...   463   \n",
       "2      487   542   499   468   448   471   442   478   480    477  ...   487   \n",
       "3      480   491   510   485   495   472   417   474   502    476  ...   491   \n",
       "4      484   502   528   489   466   481   402   478   487    468  ...   488   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   ...   \n",
       "1995   490   505   503   474   463   461   519   476   518    467  ...   467   \n",
       "1996   480   475   476   480   495   482   515   479   480    484  ...   464   \n",
       "1997   480   517   631   470   485   474   535   476   493    466  ...   501   \n",
       "1998   484   481   505   478   542   477   518   477   510    472  ...   487   \n",
       "1999   474   493   469   486   521   475   494   479   481    473  ...   467   \n",
       "\n",
       "      V492  V493  V494  V495  V496  V497  V498  V499  V500  \n",
       "0      481   477   485   511   485   481   479   475   496  \n",
       "1      478   487   338   513   486   483   492   510   517  \n",
       "2      481   492   650   506   501   480   489   499   498  \n",
       "3      480   474   572   454   469   475   482   494   461  \n",
       "4      479   452   435   486   508   481   504   495   511  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "1995   479   449   588   499   506   475   463   507   501  \n",
       "1996   474   473   424   454   570   476   493   465   485  \n",
       "1997   483   479   687   488   488   483   500   523   481  \n",
       "1998   483   526   750   486   529   484   473   527   485  \n",
       "1999   476   508   449   463   533   481   489   516   516  \n",
       "\n",
       "[2000 rows x 500 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list()\n",
    "for column_int in uci_train.columns.to_list():\n",
    "    if column_int < 10:\n",
    "        column_names.append(\"V00{}\".format(column_int + 1))\n",
    "    elif ((column_int >= 10) & (column_int < 100)):\n",
    "        column_names.append(\"V0{}\".format(column_int + 1))\n",
    "    elif column_int >= 100:\n",
    "        column_names.append(\"V{}\".format(column_int + 1))\n",
    "\n",
    "#print(column_names)\n",
    "\n",
    "X_train = pd.DataFrame(data = uci_train.values, columns = column_names)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe8b9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(uci_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e9ca2",
   "metadata": {},
   "source": [
    "We can change the label column from numeric to categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb599543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class\n",
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "...    ...\n",
       "1995     1\n",
       "1996     0\n",
       "1997     0\n",
       "1998     1\n",
       "1999     1\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.DataFrame(data = uci_target.values, columns = [\"class\"])\n",
    "\n",
    "# change the class label -1 to 0\n",
    "y_train.replace({-1 : 0}, inplace = True)\n",
    "y_train[\"class\"] = y_train[\"class\"].astype(\"category\")\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53024b87",
   "metadata": {},
   "source": [
    "It's more handy to concatenate the features dataframe and the target labels dataframe together. In this way, the whole \"cleaned\" dataset can be passed to the following function using only 2 arguments (the whole dataset and the name of the target column, because we assume that all other columns of the whole dataset contain the features to be searched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9847ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d1793",
   "metadata": {},
   "source": [
    "## Searching the most informative subset of features\n",
    "I have split the search in more functions, to make each step clearer. Let's see the \"ingredients\" of the research algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8e70e",
   "metadata": {},
   "source": [
    "### Compute the correlation metrics\n",
    "We need a metric to compute the feature-to-feature and feature-to-target correlation. For what regards the feature-to-target correlation, if the target variable were continuous, we could apply the Pearson correlation coefficient, that measures the linear relationship between two random variables. The Pearson correlation coefficient is the ratio between the covariance of two variables and the product of their standard deviations. Like other correlation coefficients, the Pearson coefficient varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact linear relationship. We're not dealing with a regression problem, but with a classification problem (i.e. the target is discrete, and here also categorical). The relationship between a binary variable and a continuous variable, can be evaluated using the point biserial correlation. \n",
    "The formula can be retrived from \"stats\" module official documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
    "\n",
    "Instead, for what regards the computation of the average feature-to-feature correlation, we can use the Pearson correlation, since all features are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e325a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointbiserial_corr(x, y):\n",
    "    \"\"\"\n",
    "    Calls the pointbiserial correlation from 'stats' module, given 2 numpy arrays.\n",
    "    The output is the absolute value of the correlation coefficient.\n",
    "    \"\"\"\n",
    "    # calculates the absolute value of the pointbiserial correlation \n",
    "    # (note the '0' item, to get only the corr value and not the p_value)\n",
    "    return abs(pointbiserialr(x, y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57ae8f",
   "metadata": {},
   "source": [
    "Note that the function above calls the pointbiserial correlation for 2 given arrays and takes the absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552469a8",
   "metadata": {},
   "source": [
    "### Merit score\n",
    "The merit is a metric associated to a subset of features, drawn from the original features space. In the numerator, we have the average feature-to-target correaltion, in the denominator the square root of the the average feature-to-feature correlation. The constant terms are based on k, which is the number of features in the considered subset.\n",
    "\n",
    "Given a subset of features, the merit is higher when the average feature-to-feature correlation is lower (selected features are generally uncorrelated to each other) and when the average feature-to-target correlation is higher (selected features are likely to impact on the tagret)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f50ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMerit(df, subset, class_label):\n",
    "    \"\"\"\n",
    "    Computes the merit score for a subset of the input features taken from the given dataframe, with the target class column.\n",
    "    \n",
    "    arguments:\n",
    "        df : pandas dataframe containing (X, y) columns all together\n",
    "        subset : lis of strings\n",
    "            list of input fetaures to be used for calculating the merit\n",
    "        class_label: str\n",
    "            column name of df, containing the class label ID\n",
    "    \"\"\"\n",
    "    X = df[subset] # get only the columns contained in the 'subset' argument\n",
    "    y = df[class_label]    \n",
    "    k = len(subset)\n",
    "\n",
    "    # average feature-class correlation\n",
    "    # applies the above defined function to all columns of the X dataframe, passing the y dataframe as common 'target'\n",
    "    corr_array = (X.apply(lambda x: pointbiserial_corr(x, y.values))).values # this is a numpy array\n",
    "    rcf = np.mean(corr_array)\n",
    "\n",
    "    # average feature-feature correlation\n",
    "    corr_df = X[subset].corr() # pairwise correlation. The first output is a dataframe\n",
    "    corr_df.values[np.tril_indices_from(corr_df.values)] = np.nan # get only the upper triangular\n",
    "    corr_df = abs(corr_df) # apply the absolute value\n",
    "    rff = corr_df.unstack().mean() # note that pandas 'mean' will skip the NaN entries. This is a scalar.\n",
    "\n",
    "    return (k * rcf) / sqrt(k + k * (k-1) * rff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc74c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008899487891043917"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example how to call the merit function\n",
    "sample_merit = getMerit(training_df, subset = [\"V500\", \"V001\", \"V003\"], class_label = \"class\" )\n",
    "sample_merit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214d175",
   "metadata": {},
   "source": [
    "### Best first search\n",
    "The algorithm has to start from a first guess of features. A good starting point is to select the feature having the highest merit. So the first subset contains only 1 feature, and this one is the \"most significant\".\n",
    "\n",
    "From the definition of the merit score, when k = 1, the merit degenerates to the value of the feature-to-class correlation, for each feature in the original feature space. We can simply apply the pointbiserial function to all columns of the whole dataset and rank the features according to their correlation score with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8465e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature is: V476 with merit: 0.2199331363610972\n"
     ]
    }
   ],
   "source": [
    "def get_best_first_search(df, class_label): \n",
    "    \"\"\"\n",
    "    Gets the feature with higher merit.\n",
    "    \n",
    "    Arguments:\n",
    "        df : pandas dataframe\n",
    "            The whole dataframe, containing both input features and class label column.\n",
    "        class_label : str\n",
    "            Column name of the class label column.\n",
    "    Return:\n",
    "        (best_feature, best_value) : tuple\n",
    "            Returns the feature with higher merit and its merit value.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns = class_label)\n",
    "    y = df[class_label]\n",
    "    corr_series = X.apply(lambda x: pointbiserial_corr(x, y.values))\n",
    "    corr_series.sort_values(ascending = False, inplace = True)\n",
    "    return corr_series.index[0], corr_series[0]\n",
    "\n",
    "best_feature, best_value = get_best_first_search(training_df, \"class\")\n",
    "print(\"Best feature is: {} with merit: {}\".format(best_feature, best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff5449",
   "metadata": {},
   "source": [
    "### Putting all together in the search function\n",
    "The following function contains the core of the algorithm. How does the search algorithm work ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f084fcd",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "The first guess is expressed as the feature with higher merit. We initialize a queue dataframe to contain the succession of subsets that will be searched, putting the feature with higher merit in the first (index 0) position. \n",
    "Already visited subsets and their score is recorded in 2 lists: this enables to track the algorithm search history. The number of backtracks is initialized to 0, since it's a counter that will be incremented in the while loop. Note that we imposed a max number of backtracks to 5. This is an arbitrary choice: we could also select another number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb41d06",
   "metadata": {},
   "source": [
    "#### Subset expansion\n",
    "The algorithm iteratively expands the subset on the top of the queue (i.e. the subset which has an associated higher \"priority\") with all other possible features and looks for the feature that increases the merit the most. In this way, at the end of the inner \"for\" loop, we obtain another subset which consists in the same subset at the previous iteration, plus the new best-performing feature drawn from the whole dataframe. If the new subset performs worse than the old one, than the search jumps back to the old one, performing litterally a \"backtrack\". Otherwise, the search continues by expanding the improved subset of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78603f4b",
   "metadata": {},
   "source": [
    "#### Stopping criteria\n",
    "The search process terminates when either the queue dataframe is empty or maximum allowed number of backtracks has been reached. Limiting the number of backtracks is something like imposing a limit to the improvement that can be reached by adding even more features to the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62aa5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_features_set(df, class_label, max_backtrack = 5):\n",
    "    \n",
    "    best_subset, best_value = get_best_first_search(df, class_label)\n",
    "    \n",
    "    # initialize the queue dataframe with the overall best merit feature\n",
    "    queue = pd.DataFrame({\"features\" : [[best_feature]], \"priority\" : best_value})\n",
    "    iteration = 0\n",
    "    visited = []\n",
    "    visited_score = []\n",
    "    n_backtrack = 0\n",
    "    \n",
    "    while ((not queue.empty) and (n_backtrack <= max_backtrack)):\n",
    "        # sort the queue dataframe according to the descending merit and get the highes set\n",
    "        queue.sort_values(by = \"priority\", ascending = False, inplace = True, ignore_index = True)\n",
    "        subset = queue.loc[0, \"features\"]\n",
    "        priority = queue.loc[0, \"priority\"]\n",
    "        queue.drop(index = 0, inplace = True)\n",
    "        \n",
    "        # check if the new found set has better performance than the previous\n",
    "        if priority < best_value:\n",
    "            n_backtrack = n_backtrack + 1 # the new subset is not improving the score\n",
    "        else:\n",
    "            best_value = priority # this means the new subset is improving the score\n",
    "            best_subset = subset\n",
    "            \n",
    "        # consider feature in the input dataframe, except the one containing the class label and the columns\n",
    "        # contained in the best subset\n",
    "        column_names = df.drop(columns = [class_label] + subset).columns.to_list() \n",
    "        \n",
    "        for column_name in column_names:\n",
    "            temp_subset = subset + [column_name]\n",
    "            # check if the actual set of variables has already been tested\n",
    "            already_visited = (any([set(temp_subset) == set(element) for element in visited]))\n",
    "            if already_visited:\n",
    "                print(\"{} already visited.\".format(set(temp_subset)))\n",
    "            else:\n",
    "                visited.append(temp_subset)\n",
    "                merit = getMerit(df, temp_subset, class_label = class_label)\n",
    "                visited_score.append(merit)\n",
    "                new_item = pd.DataFrame({\"features\" : [temp_subset], \"priority\" : merit})\n",
    "                queue = queue.append(new_item, ignore_index = True)\n",
    "                del new_item\n",
    "        iteration = iteration + 1 \n",
    "        print(\"Search iter # {}\".format(iteration))\n",
    "    return (best_subset, best_value, visited, visited_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ea89e",
   "metadata": {},
   "source": [
    "## Running the algorithm\n",
    "Running the search function may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09ccf58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search iter # 1\n",
      "Search iter # 2\n",
      "Search iter # 3\n",
      "Search iter # 4\n",
      "Search iter # 5\n",
      "Search iter # 6\n",
      "Search iter # 7\n",
      "Search iter # 8\n",
      "Search iter # 9\n",
      "Search iter # 10\n",
      "Search iter # 11\n",
      "Search iter # 12\n",
      "Search iter # 13\n",
      "Search iter # 14\n",
      "Search iter # 15\n",
      "Search iter # 16\n",
      "Search iter # 17\n",
      "Search iter # 18\n",
      "Search iter # 19\n",
      "Search iter # 20\n",
      "Search iter # 21\n",
      "Search iter # 22\n",
      "Search iter # 23\n",
      "Search iter # 24\n",
      "Search iter # 25\n",
      "Search iter # 26\n",
      "Search iter # 27\n",
      "Search iter # 28\n",
      "Search iter # 29\n",
      "Search iter # 30\n",
      "Search iter # 31\n",
      "Search iter # 32\n",
      "Search iter # 33\n",
      "Search iter # 34\n",
      "Search iter # 35\n",
      "Search iter # 36\n",
      "Search iter # 37\n",
      "Search iter # 38\n",
      "Search iter # 39\n",
      "Search iter # 40\n",
      "Search iter # 41\n",
      "Search iter # 42\n",
      "Search iter # 43\n",
      "Search iter # 44\n",
      "Search iter # 45\n",
      "Search iter # 46\n",
      "Search iter # 47\n",
      "Search iter # 48\n",
      "Search iter # 49\n",
      "Search iter # 50\n",
      "Search iter # 51\n",
      "Search iter # 52\n",
      "Search iter # 53\n",
      "Search iter # 54\n",
      "Search iter # 55\n",
      "Search iter # 56\n",
      "Search iter # 57\n",
      "Search iter # 58\n",
      "Search iter # 59\n",
      "Search iter # 60\n",
      "Search iter # 61\n",
      "{'V287', 'V330', 'V495', 'V138', 'V106', 'V378', 'V162', 'V057', 'V379', 'V249', 'V476', 'V056', 'V044', 'V286', 'V455', 'V197', 'V206', 'V445', 'V446', 'V212', 'V086', 'V415', 'V164', 'V227', 'V411', 'V299', 'V443', 'V153', 'V349', 'V413', 'V385', 'V432', 'V431', 'V062', 'V482', 'V426', 'V065', 'V049', 'V228', 'V120', 'V297', 'V200', 'V412', 'V047', 'V454', 'V497', 'V425', 'V242', 'V334', 'V348', 'V283', 'V011', 'V185', 'V205', 'V473', 'V117', 'V324', 'V421', 'V137', 'V150', 'V337', 'V279'} already visited.\n",
      "Search iter # 62\n",
      "Search iter # 63\n",
      "{'V287', 'V330', 'V495', 'V138', 'V106', 'V378', 'V162', 'V057', 'V379', 'V249', 'V476', 'V056', 'V044', 'V286', 'V455', 'V197', 'V206', 'V445', 'V446', 'V212', 'V086', 'V415', 'V164', 'V227', 'V411', 'V299', 'V443', 'V153', 'V349', 'V413', 'V385', 'V432', 'V431', 'V482', 'V426', 'V065', 'V049', 'V228', 'V120', 'V297', 'V200', 'V412', 'V047', 'V454', 'V497', 'V425', 'V242', 'V334', 'V348', 'V283', 'V011', 'V185', 'V205', 'V473', 'V117', 'V324', 'V421', 'V137', 'V150', 'V337', 'V279'} already visited.\n",
      "Search iter # 64\n",
      "{'V287', 'V330', 'V495', 'V138', 'V106', 'V378', 'V162', 'V057', 'V379', 'V249', 'V476', 'V056', 'V044', 'V286', 'V455', 'V197', 'V206', 'V445', 'V310', 'V446', 'V212', 'V086', 'V415', 'V164', 'V227', 'V411', 'V299', 'V443', 'V153', 'V349', 'V413', 'V385', 'V432', 'V431', 'V062', 'V482', 'V426', 'V065', 'V049', 'V228', 'V120', 'V297', 'V200', 'V412', 'V047', 'V454', 'V497', 'V425', 'V242', 'V334', 'V348', 'V283', 'V011', 'V185', 'V205', 'V473', 'V117', 'V324', 'V421', 'V137', 'V150', 'V337', 'V279'} already visited.\n",
      "Search iter # 65\n",
      "{'V287', 'V330', 'V495', 'V138', 'V106', 'V378', 'V162', 'V057', 'V379', 'V249', 'V476', 'V056', 'V044', 'V286', 'V455', 'V197', 'V206', 'V445', 'V310', 'V446', 'V212', 'V086', 'V415', 'V227', 'V411', 'V299', 'V443', 'V153', 'V349', 'V413', 'V385', 'V432', 'V431', 'V062', 'V482', 'V426', 'V065', 'V049', 'V228', 'V120', 'V297', 'V200', 'V412', 'V047', 'V454', 'V497', 'V425', 'V242', 'V334', 'V348', 'V283', 'V011', 'V185', 'V205', 'V473', 'V117', 'V324', 'V421', 'V137', 'V150', 'V337', 'V279'} already visited.\n",
      "{'V287', 'V330', 'V495', 'V138', 'V106', 'V378', 'V162', 'V057', 'V379', 'V249', 'V476', 'V056', 'V044', 'V286', 'V455', 'V197', 'V206', 'V445', 'V310', 'V446', 'V212', 'V086', 'V415', 'V164', 'V227', 'V411', 'V299', 'V443', 'V153', 'V349', 'V413', 'V385', 'V432', 'V431', 'V482', 'V426', 'V065', 'V049', 'V228', 'V120', 'V297', 'V200', 'V412', 'V047', 'V454', 'V497', 'V425', 'V242', 'V334', 'V348', 'V283', 'V011', 'V185', 'V205', 'V473', 'V117', 'V324', 'V421', 'V137', 'V150', 'V337', 'V279'} already visited.\n",
      "Search iter # 66\n"
     ]
    }
   ],
   "source": [
    "(best_subset, best_value, visited, visited_score) = search_best_features_set(training_df, \"class\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b6d844f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e0413e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3034575055824045"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca91e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem dimensionality has been reduced from 501 to 60 features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Problem dimensionality has been reduced from {} to {} features.\".format(training_df.shape[1], len(best_subset))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b570537",
   "metadata": {},
   "source": [
    "## References\n",
    "This work is based on the Python implementation by Johannes Fisher of the same method, that can be found here: https://johfischer.com/2021/08/06/correlation-based-feature-selection-in-python-from-scratch. \n",
    "\n",
    "Dataset repository: https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON\n",
    "\n",
    "Hall, M. A. (2000). Correlation-based feature selection of discrete and numeric class machine learning.\n",
    "Isabelle Guyon, Steve R. Gunn, Asa Ben-Hur, Gideon Dror, 2004. Result analysis of the NIPS 2003 feature selection challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de8256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "449.841px",
    "left": "1026.36px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
